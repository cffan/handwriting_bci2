{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This notebook generates synthetic sentences to agument the RNN's training data (for BOTH of the train/test partitions and\n",
    "#ALL ten sessions). Step 3 utilizes the data labels created during Step 2 to rearrange the data into new sentences. \n",
    "#The output of Step 3 is a set of .tfrecord files that are mixed together with the real data during RNN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#suppress all tensorflow warnings (largely related to compatability with v2)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.decomposition \n",
    "from characterDefinitions import getHandwritingCharacterDefinitions\n",
    "from makeSyntheticSentences import generateCharacterSequences, extractCharacterSnippets, addSingleLetterSnippets\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor, wait\n",
    "import datetime\n",
    "from dataPreprocessing import normalizeSentenceDataCube\n",
    "\n",
    "#point this towards the top level dataset directory\n",
    "rootDir = '/scratch/users/stfan' + '/handwritingBCIData/'\n",
    "\n",
    "#define which datasets to process\n",
    "dataDirs = ['t5.2019.05.08','t5.2019.12.09','t5.2019.12.11','t5.2019.12.18',\n",
    "            't5.2019.12.20','t5.2020.01.06','t5.2020.01.08','t5.2020.01.13','t5.2020.01.15']\n",
    "\n",
    "maxLengths = [\n",
    "    5334*2, 5522*2, 4117*2, 4783*2, 4207*2, 3239*2, 3944*2, 3105*2, 3631*2\n",
    "]\n",
    "\n",
    "# dataDirs = ['t5.2019.11.25']\n",
    "# maxLengths = [6938*2]\n",
    "\n",
    "#construct synthetic data for both training partitions\n",
    "cvParts = ['HeldOutBlocks', \n",
    "           #'HeldOutTrials']\n",
    "          ]\n",
    "\n",
    "#defines the list of all 31 characters and what to call them\n",
    "charDef = getHandwritingCharacterDefinitions()\n",
    "\n",
    "#saves all synthetic sentences & snippet libraries in this folder\n",
    "outputDir = rootDir + 'RNNTrainingSteps/Step3_SyntheticSentences_tf2/'\n",
    "if not os.path.isdir(outputDir):\n",
    "    os.mkdir(outputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing t5.2019.05.08\n",
      "--HeldOutTrials\n",
      "Processing t5.2019.11.25\n",
      "--HeldOutTrials\n",
      "Processing t5.2019.12.09\n",
      "--HeldOutTrials\n",
      "Processing t5.2019.12.11\n",
      "--HeldOutTrials\n",
      "Processing t5.2019.12.18\n",
      "--HeldOutTrials\n",
      "Processing t5.2019.12.20\n",
      "--HeldOutTrials\n",
      "Processing t5.2020.01.06\n",
      "--HeldOutTrials\n",
      "Processing t5.2020.01.08\n",
      "--HeldOutTrials\n",
      "Processing t5.2020.01.13\n",
      "--HeldOutTrials\n",
      "Processing t5.2020.01.15\n",
      "--HeldOutTrials\n"
     ]
    }
   ],
   "source": [
    "#First, we generate snippet libraries for each dataset by cutting out each letter from each sentence. These can then\n",
    "#be re-arranged into new sequences. \n",
    "for dataDir in dataDirs:\n",
    "    print('Processing ' + dataDir)\n",
    "    \n",
    "    for cvPart in cvParts:\n",
    "        print('--' + cvPart)\n",
    "        \n",
    "        #load datasets and train/test partition\n",
    "        sentenceDat = scipy.io.loadmat(rootDir+'Datasets/'+dataDir+'/sentences.mat')\n",
    "        singleLetterDat = scipy.io.loadmat(rootDir+'Datasets/'+dataDir+'/singleLetters.mat')\n",
    "        twCubes = scipy.io.loadmat(rootDir+'RNNTrainingSteps/Step1_TimeWarping/'+dataDir+'_warpedCubes.mat')\n",
    "        \n",
    "        cvPartFile = scipy.io.loadmat(rootDir+'RNNTrainingSteps/trainTestPartitions_'+cvPart+'.mat')\n",
    "        trainPartitionIdx = cvPartFile[dataDir+'_train']\n",
    "        \n",
    "        #the last two sessions have hashmarks (#) to indicate that T5 should take a brief pause\n",
    "        #here we remove these from the sentence prompts, otherwise the code below will get confused (because # isn't a character)\n",
    "        for x in range(sentenceDat['sentencePrompt'].shape[0]):\n",
    "            sentenceDat['sentencePrompt'][x,0][0] = sentenceDat['sentencePrompt'][x,0][0].replace('#','')\n",
    "        \n",
    "        #normalize the neural activity cube\n",
    "        neuralCube = normalizeSentenceDataCube(sentenceDat, singleLetterDat)\n",
    "        \n",
    "        #load labels\n",
    "        labels = scipy.io.loadmat(rootDir + 'RNNTrainingSteps/Step2_HMMLabels/'+cvPart+'/'+dataDir+'_timeSeriesLabels.mat')\n",
    "\n",
    "        #cut out character snippets from the data for augmentation\n",
    "        snippetDict = extractCharacterSnippets(labels['letterStarts'], \n",
    "                                               labels['blankWindows'], \n",
    "                                               neuralCube, \n",
    "                                               sentenceDat['sentencePrompt'][:,0], \n",
    "                                               sentenceDat['numTimeBinsPerSentence'][:,0], \n",
    "                                               trainPartitionIdx, \n",
    "                                               charDef)\n",
    "\n",
    "        #add single letter examples\n",
    "        snippetDict = addSingleLetterSnippets(snippetDict, \n",
    "                                              singleLetterDat, \n",
    "                                              twCubes, \n",
    "                                              charDef)\n",
    "\n",
    "        #save results\n",
    "        if not os.path.isdir(rootDir + 'RNNTrainingSteps/Step3_SyntheticSentences_random_sentence_bos/'+cvPart):\n",
    "            os.mkdir(rootDir + 'RNNTrainingSteps/Step3_SyntheticSentences_random_sentence_bos/'+cvPart)\n",
    "        scipy.io.savemat(rootDir + 'RNNTrainingSteps/Step3_SyntheticSentences_random_sentence_bos/'+cvPart+'/'+dataDir+'_snippets.mat', snippetDict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing t5.2019.05.08\n",
      "--HeldOutBlocks\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "Processing t5.2019.12.09\n",
      "--HeldOutBlocks\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "Processing t5.2019.12.11\n",
      "--HeldOutBlocks\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "Processing t5.2019.12.18\n",
      "--HeldOutBlocks\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "Processing t5.2019.12.20\n",
      "--HeldOutBlocks\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "Processing t5.2020.01.06\n",
      "--HeldOutBlocks\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "Processing t5.2020.01.08\n",
      "--HeldOutBlocks\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "Processing t5.2020.01.13\n",
      "--HeldOutBlocks\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "Processing t5.2020.01.15\n",
      "--HeldOutBlocks\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#Now we use the above snippet libraries to make synthetic data for each dataset and train/test partition.\n",
    "\n",
    "#'nParallelProcesses' specifies how many parallel processes to use when generating synthetic data (to speed things up).\n",
    "#Decrease if it uses too much memory on your machine. (10 uses ~30 GB of RAM)\n",
    "nParallelProcesses = 8\n",
    "\n",
    "for nSteps, dataDir in zip(maxLengths, dataDirs):\n",
    "    print('Processing ' + dataDir)\n",
    "    \n",
    "    for cvPart in cvParts:\n",
    "        print('--' + cvPart)\n",
    "        \n",
    "        currOutputDir = outputDir+cvPart+'/'+dataDir+'_syntheticSentences'\n",
    "        bashDir = rootDir+'bashScratch'\n",
    "        repoDir = os.getcwd()\n",
    "\n",
    "        Path(currOutputDir).mkdir(parents=True, exist_ok=True)\n",
    "        Path(bashDir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        args = {}\n",
    "        args['nSentences'] = 64\n",
    "        args['nSteps'] = nSteps\n",
    "        args['binSize'] = 2\n",
    "        args['wordListFile'] = repoDir+'/wordList/google-10000-english-usa.txt' #from https://github.com/first20hours/google-10000-english\n",
    "        args['rareWordFile'] = repoDir+'/wordList/rareWordIdx.mat'\n",
    "        args['snippetFile'] = rootDir + 'RNNTrainingSteps/Step3_SyntheticSentences/'+cvPart+'/'+dataDir+'_snippets.mat'\n",
    "        args['accountForPenState'] = 1\n",
    "        args['charDef'] = getHandwritingCharacterDefinitions()\n",
    "        args['seed'] = datetime.datetime.now().microsecond\n",
    "        \n",
    "        with open('/scratch/users/stfan/handwritingBCIData/webTextSentences.txt', 'r') as f:\n",
    "            sentences = [l.strip() for l in f.readlines()]\n",
    "        args['sentenceList'] = sentences\n",
    "\n",
    "        argList = []\n",
    "        for x in range(80):\n",
    "            newArgs = args.copy()\n",
    "            newArgs['saveFile'] = currOutputDir+'/bat_'+str(x)+'.tfrecord'\n",
    "            newArgs['seed'] += x\n",
    "            argList.append(newArgs)\n",
    "            \n",
    "        futures = []\n",
    "        with ProcessPoolExecutor(nParallelProcesses) as p:\n",
    "            for arg in argList:\n",
    "                futures.append(p.submit(generateCharacterSequences, arg))\n",
    "\n",
    "        results = [f.result() for f in futures]\n",
    "        print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.89646584 -0.30436826  0.41243026 ... -0.5551781   0.80052704\n",
      "   0.7129129 ]\n",
      " [-0.27373    -0.30436826 -0.38925537 ...  0.07325782 -0.25691307\n",
      "   0.7129129 ]\n",
      " [ 0.89646584 -0.30436826  0.41243026 ... -0.5551781  -0.25691307\n",
      "  -0.65961415]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]], shape=(6938, 192), dtype=float32) (6938, 192)\n",
      "tf.Tensor(66, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from glob import glob\n",
    "\n",
    "dataDir = '/scratch/users/stfan/handwritingBCIData/RNNTrainingSteps/Step3_SyntheticSentences_tf2/HeldOutBlocks/t5.2019.11.25_syntheticSentences/'\n",
    "files = glob(dataDir + '*.tfrecord')\n",
    "# print(files)\n",
    "dataset = tf.data.TFRecordDataset(files)\n",
    "\n",
    "nInputFeatures = 192\n",
    "nClasses = 31\n",
    "maxSeqElements = 500\n",
    "\n",
    "datasetFeatures = {\"inputFeatures\": tf.io.FixedLenSequenceFeature([nInputFeatures], tf.float32, allow_missing=True),\n",
    "#             \"classLabelsOneHot\": tf.io.FixedLenSequenceFeature([nClasses], tf.float32, allow_missing=True),\n",
    "            \"newClassSignal\": tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n",
    "            \"ceMask\": tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n",
    "            \"seqClassIDs\": tf.io.FixedLenFeature((maxSeqElements), tf.int64),\n",
    "            \"nTimeSteps\": tf.io.FixedLenFeature((), tf.int64),\n",
    "            \"nSeqElements\": tf.io.FixedLenFeature((), tf.int64),\n",
    "            \"transcription\": tf.io.FixedLenFeature((maxSeqElements), tf.int64)}\n",
    "\n",
    "def parseDatasetFunction(exampleProto):\n",
    "    return tf.io.parse_single_example(exampleProto, datasetFeatures)\n",
    "\n",
    "dataset = dataset.map(parseDatasetFunction).shuffle(10)\n",
    "# dataset = dataset.padded_batch(1)\n",
    "datIter = iter(dataset)\n",
    "\n",
    "for i, d in enumerate(datIter):\n",
    "    print(d['inputFeatures'], d['inputFeatures'].shape)\n",
    "    print(d['nSeqElements'])\n",
    "    print(tf.reduce_min(d['seqClassIDs'][:d['nSeqElements']]))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.7",
   "language": "python",
   "name": "py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
